import { useState } from 'react';
import { useTranslation } from 'react-i18next';
import axios from 'axios';
import { Demo, ParameterSlider, Button, Chart, AlgorithmHeader, PlayIcon, ResetIcon } from '@common/modules/MDX/MDXComponents';
import { useLogger } from '@common/modules/Logger';
import { useExceptions, ExceptionType, ExceptionSeverity } from '@common/modules/Exceptions';

export const DecisionTreeDemo = () => {
  const { t } = useTranslation();
  const logger = useLogger();
  const { captureException } = useExceptions();
  const [maxDepth, setMaxDepth] = useState(5);
  const [minSamplesSplit, setMinSamplesSplit] = useState(2);
  const [loading, setLoading] = useState(false);
  const [result, setResult] = useState(null);

  const runTraining = async () => {
    setLoading(true);
    logger.info('å¼€å§‹è®­ç»ƒå†³ç­–æ ‘æ¨¡å‹', { maxDepth, minSamplesSplit });
    
    try {
      const response = await axios.post('/api/algorithms/supervised/decision-tree/train', {
        max_depth: maxDepth,
        min_samples_split: minSamplesSplit,
      });
      setResult(response.data);
      logger.info('å†³ç­–æ ‘æ¨¡å‹è®­ç»ƒæˆåŠŸ', { result: response.data });
    } catch (error) {
      logger.error('å†³ç­–æ ‘æ¨¡å‹è®­ç»ƒå¤±è´¥', { error: error.message, maxDepth, minSamplesSplit });
      captureException(error, {
        type: ExceptionType.NETWORK,
        severity: ExceptionSeverity.HIGH,
        context: { 
          algorithm: 'decision-tree',
          maxDepth,
          minSamplesSplit,
          message: 'è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥åç«¯æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ'
        }
      });
    } finally {
      setLoading(false);
    }
  };

  const resetParameters = () => {
    setMaxDepth(5);
    setMinSamplesSplit(2);
    setResult(null);
  };

  return (
    <Demo title="âš¡ äº¤äº’å¼å®éªŒ">
      <div className="space-y-6">
        <div className="bg-gray-50 dark:bg-gray-800 p-4 rounded-lg">
          <h4 className="font-semibold mb-4 text-gray-900 dark:text-gray-100">ğŸ“Š å‚æ•°è°ƒæ•´</h4>
          
          <ParameterSlider
            label="æœ€å¤§æ·±åº¦ (Max Depth)"
            value={maxDepth}
            onChange={setMaxDepth}
            min={1}
            max={20}
            step={1}
          />
          
          <ParameterSlider
            label="æœ€å°åˆ†è£‚æ ·æœ¬æ•° (Min Samples Split)"
            value={minSamplesSplit}
            onChange={setMinSamplesSplit}
            min={2}
            max={20}
            step={1}
          />

          <div className="flex gap-3 mt-6">
            <Button onClick={runTraining} icon={PlayIcon} disabled={loading}>
              {loading ? 'è®­ç»ƒä¸­...' : 'è¿è¡Œè®­ç»ƒ'}
            </Button>
            <Button onClick={resetParameters} variant="secondary" icon={ResetIcon}>
              é‡ç½®å‚æ•°
            </Button>
          </div>
        </div>

        {result && (
          <div className="space-y-4">
            <div className="bg-green-50 dark:bg-green-900/20 p-4 rounded-lg">
              <h4 className="font-semibold mb-2 text-gray-900 dark:text-gray-100">ğŸŒ³ è®­ç»ƒç»“æœ</h4>
              <div className="grid grid-cols-2 gap-4 text-sm">
                <div>
                  <span className="text-gray-600 dark:text-gray-400">å‡†ç¡®ç‡ (Accuracy):</span>
                  <span className="ml-2 font-mono font-semibold">{result.accuracy?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">æ ‘æ·±åº¦:</span>
                  <span className="ml-2 font-mono font-semibold">{result.tree_depth}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">å¶å­èŠ‚ç‚¹æ•°:</span>
                  <span className="ml-2 font-mono font-semibold">{result.n_leaves}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">F1 Score:</span>
                  <span className="ml-2 font-mono font-semibold">{result.f1_score?.toFixed(4)}</span>
                </div>
              </div>
            </div>

            {result.plot_data && (
              <Chart
                data={result.plot_data}
                layout={{
                  title: 'å†³ç­–æ ‘åˆ†ç±»ç»“æœ',
                  xaxis: { title: 'Feature 1' },
                  yaxis: { title: 'Feature 2' },
                  height: 400,
                }}
              />
            )}
          </div>
        )}
      </div>
    </Demo>
  );
};

<AlgorithmHeader 
  title="å†³ç­–æ ‘ Decision Tree"
  difficulty="intermediate"
  stars={3}
  time={25}
/>

## 1ï¸âƒ£ ç®—æ³•æ¦‚è§ˆ

**å†³ç­–æ ‘**ï¼ˆDecision Treeï¼‰æ˜¯ä¸€ç§åŸºäºæ ‘å½¢ç»“æ„çš„ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œå¯ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡ã€‚å®ƒé€šè¿‡ä¸€ç³»åˆ—è§„åˆ™ï¼ˆif-then è¯­å¥ï¼‰å°†æ•°æ®é›†é€’å½’åœ°åˆ’åˆ†ä¸ºæ›´å°çš„å­é›†ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªç±»ä¼¼æµç¨‹å›¾çš„æ ‘å½¢å†³ç­–æ¨¡å‹ã€‚

### æ ¸å¿ƒæ€æƒ³

- ä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œé€‰æ‹©**æœ€ä¼˜ç‰¹å¾**è¿›è¡Œåˆ†è£‚
- ä½¿ç”¨**ä¿¡æ¯å¢ç›Š**ã€**åŸºå°¼æŒ‡æ•°**ç­‰æŒ‡æ ‡è¯„ä¼°åˆ†è£‚è´¨é‡
- é€’å½’æ„å»ºå­æ ‘ï¼Œç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶
- æ–°æ ·æœ¬æ²¿ç€æ ‘ä»æ ¹èŠ‚ç‚¹åˆ°å¶èŠ‚ç‚¹ï¼Œå¾—åˆ°é¢„æµ‹ç»“æœ

### åº”ç”¨åœºæ™¯

- ğŸ¥ **åŒ»ç–—è¯Šæ–­**ï¼šæ ¹æ®ç—‡çŠ¶é€æ­¥ç¼©å°ç–¾ç—…èŒƒå›´
- ğŸ’° **ä¿¡ç”¨è¯„ä¼°**ï¼šè¯„ä¼°è´·æ¬¾ç”³è¯·çš„é£é™©ç­‰çº§
- ğŸ¯ **å®¢æˆ·åˆ†ç±»**ï¼šæ ¹æ®è¡Œä¸ºç‰¹å¾è¿›è¡Œç”¨æˆ·ç”»åƒ
- ğŸ® **æ¸¸æˆ AI**ï¼šNPC å†³ç­–é€»è¾‘

---

## 2ï¸âƒ£ å‘å±•å†å²

- **1963å¹´** - **E. B. Hunt** ç­‰äººæå‡º CLSï¼ˆConcept Learning Systemï¼‰ç®—æ³•
- **1979å¹´** - **J. Ross Quinlan** æå‡º **ID3 ç®—æ³•**ï¼ˆIterative Dichotomiser 3ï¼‰
- **1986å¹´** - Quinlan æ”¹è¿› ID3ï¼Œæå‡º **C4.5 ç®—æ³•**ï¼Œå¤„ç†è¿ç»­ç‰¹å¾å’Œç¼ºå¤±å€¼
- **1984å¹´** - **CART ç®—æ³•**ï¼ˆClassification and Regression Treesï¼‰ç”± Breiman ç­‰äººæå‡º
- **21ä¸–çºª** - å†³ç­–æ ‘æˆä¸ºé›†æˆå­¦ä¹ ï¼ˆéšæœºæ£®æ—ã€GBDTã€XGBoostï¼‰çš„åŸºç¡€

---

## 3ï¸âƒ£ æ•°å­¦åŸç†

### ä¿¡æ¯ç†µï¼ˆEntropyï¼‰

è¡¡é‡æ•°æ®é›†çš„**ä¸ç¡®å®šæ€§**æˆ–**æ··ä¹±ç¨‹åº¦**ï¼š

```math
H(D) = -\sum_{k=1}^{K}p_k\log_2(p_k)
```

å…¶ä¸­ $p_k$ æ˜¯ç¬¬ $k$ ç±»æ ·æœ¬çš„æ¯”ä¾‹ã€‚

- ç†µè¶Šå¤§ï¼Œæ•°æ®è¶Šæ··ä¹±
- ç†µè¶Šå°ï¼Œæ•°æ®è¶Šçº¯å‡€
- å½“æ•°æ®é›†åªæœ‰ä¸€ä¸ªç±»åˆ«æ—¶ï¼Œ$H(D) = 0$

### ä¿¡æ¯å¢ç›Šï¼ˆInformation Gainï¼‰

ä½¿ç”¨ç‰¹å¾ $A$ åˆ’åˆ†æ•°æ®é›†åï¼Œ**ç†µçš„å‡å°‘é‡**ï¼š

$$
IG(D, A) = H(D) - \sum_{v=1}^{V}\frac{|D_v|}{|D|}H(D_v)
$$

å…¶ä¸­ $D_v$ æ˜¯ç‰¹å¾ $A$ å–å€¼ä¸º $v$ çš„æ ·æœ¬å­é›†ã€‚

**ID3 ç®—æ³•**é€‰æ‹©ä¿¡æ¯å¢ç›Šæœ€å¤§çš„ç‰¹å¾è¿›è¡Œåˆ†è£‚ã€‚

### ä¿¡æ¯å¢ç›Šç‡ï¼ˆGain Ratioï¼‰

ä¸ºé¿å…åå‘å–å€¼è¾ƒå¤šçš„ç‰¹å¾ï¼Œ**C4.5 ç®—æ³•**ä½¿ç”¨ä¿¡æ¯å¢ç›Šç‡ï¼š

```math
GainRatio(D, A) = \frac{IG(D, A)}{IV(A)}
```

å…¶ä¸­ $IV(A) = -\sum_{v=1}^{V}\frac{|D_v|}{|D|}\log_2\frac{|D_v|}{|D|}$ æ˜¯å›ºæœ‰å€¼ï¼ˆIntrinsic Valueï¼‰ã€‚

### åŸºå°¼æŒ‡æ•°ï¼ˆGini Indexï¼‰

**CART ç®—æ³•**ä½¿ç”¨åŸºå°¼æŒ‡æ•°è¡¡é‡æ•°æ®é›†çš„ä¸çº¯åº¦ï¼š

```math
Gini(D) = 1 - \sum_{k=1}^{K}p_k^2
```

åŸºå°¼æŒ‡æ•°è¶Šå°ï¼Œæ•°æ®é›†è¶Šçº¯å‡€ã€‚é€‰æ‹©ä½¿åŸºå°¼æŒ‡æ•°ä¸‹é™æœ€å¤šçš„ç‰¹å¾è¿›è¡Œåˆ†è£‚ã€‚

---

## 4ï¸âƒ£ äº¤äº’å¼å®éªŒ âš¡

è°ƒæ•´å†³ç­–æ ‘çš„è¶…å‚æ•°ï¼Œè§‚å¯Ÿå¯¹æ¨¡å‹å¤æ‚åº¦å’Œæ€§èƒ½çš„å½±å“ï¼š

<DecisionTreeDemo />

---

## 5ï¸âƒ£ ä»£ç å®ç°

### Python å®ç°ï¼ˆscikit-learnï¼‰

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn import tree
import matplotlib.pyplot as plt

# åŠ è½½æ•°æ®
from sklearn.datasets import load_iris
iris = load_iris()
X, y = iris.data, iris.target

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ›å»ºå†³ç­–æ ‘åˆ†ç±»å™¨
model = DecisionTreeClassifier(
    criterion='gini',        # åˆ†è£‚æ ‡å‡†ï¼š'gini' æˆ– 'entropy'
    max_depth=5,             # æœ€å¤§æ·±åº¦
    min_samples_split=2,     # åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=1,      # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    random_state=42
)

# è®­ç»ƒæ¨¡å‹
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
print(f"æ ‘æ·±åº¦: {model.tree_.max_depth}")
print(f"å¶å­èŠ‚ç‚¹æ•°: {model.tree_.n_leaves}")

# å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(20, 10))
tree.plot_tree(model, 
               feature_names=iris.feature_names,
               class_names=iris.target_names,
               filled=True, 
               rounded=True)
plt.show()
```

### å†³ç­–æ ‘å›å½’

```python
from sklearn.tree import DecisionTreeRegressor

# å›å½’ä»»åŠ¡
model = DecisionTreeRegressor(
    max_depth=5,
    min_samples_split=10,
    random_state=42
)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

---

## 6ï¸âƒ£ å‰ªæç­–ç•¥

å†³ç­–æ ‘å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œéœ€è¦é€šè¿‡**å‰ªæ**æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼š

### é¢„å‰ªæï¼ˆPre-Pruningï¼‰

åœ¨æ„å»ºè¿‡ç¨‹ä¸­æå‰åœæ­¢æ ‘çš„ç”Ÿé•¿ï¼š

```python
model = DecisionTreeClassifier(
    max_depth=5,              # é™åˆ¶æœ€å¤§æ·±åº¦
    min_samples_split=10,     # åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=5,       # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    max_features=10,          # åˆ†è£‚æ—¶è€ƒè™‘çš„æœ€å¤§ç‰¹å¾æ•°
    max_leaf_nodes=20         # æœ€å¤§å¶èŠ‚ç‚¹æ•°
)
```

### åå‰ªæï¼ˆPost-Pruningï¼‰

å…ˆæ„å»ºå®Œæ•´çš„æ ‘ï¼Œå†è‡ªåº•å‘ä¸Šå‰ªæï¼š

```python
# scikit-learn ä½¿ç”¨ cost complexity pruning
# é€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜ ccp_alpha

path = model.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas

# åœ¨ä¸åŒ alpha ä¸‹è®­ç»ƒæ¨¡å‹
models = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=42)
    clf.fit(X_train, y_train)
    models.append(clf)

# é€‰æ‹©åœ¨éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹
best_model = models[np.argmax(train_scores)]
```

---

## 7ï¸âƒ£ ä¼˜ç¼ºç‚¹åˆ†æ

### âœ… ä¼˜ç‚¹

- **å¯è§£é‡Šæ€§å¼º**ï¼šæ ‘å½¢ç»“æ„ç›´è§‚ï¼Œæ˜“äºç†è§£å’Œå¯è§†åŒ–
- **æ— éœ€ç‰¹å¾ç¼©æ”¾**ï¼šå¯¹ç‰¹å¾é‡çº²ä¸æ•æ„Ÿ
- **å¤„ç†éçº¿æ€§å…³ç³»**ï¼šèƒ½æ•æ‰å¤æ‚çš„éçº¿æ€§æ¨¡å¼
- **æ”¯æŒå¤šè¾“å‡º**ï¼šå¯åŒæ—¶é¢„æµ‹å¤šä¸ªç›®æ ‡
- **å¤„ç†ç¼ºå¤±å€¼**ï¼šéƒ¨åˆ†å®ç°ï¼ˆå¦‚ XGBoostï¼‰èƒ½è‡ªåŠ¨å¤„ç†ç¼ºå¤±å€¼

### âŒ ç¼ºç‚¹

- **å®¹æ˜“è¿‡æ‹Ÿåˆ**ï¼šå°¤å…¶æ˜¯æ·±åº¦è¾ƒå¤§æ—¶
- **ä¸ç¨³å®š**ï¼šæ•°æ®çš„å¾®å°å˜åŒ–å¯èƒ½å¯¼è‡´å®Œå…¨ä¸åŒçš„æ ‘ç»“æ„
- **å±€éƒ¨æœ€ä¼˜**ï¼šè´ªå¿ƒç®—æ³•å¯èƒ½æ‰¾ä¸åˆ°å…¨å±€æœ€ä¼˜è§£
- **åå‘å¤šå€¼ç‰¹å¾**ï¼šæŸäº›åˆ†è£‚æ ‡å‡†åå‘å–å€¼è¾ƒå¤šçš„ç‰¹å¾
- **è¡¨è¾¾èƒ½åŠ›æœ‰é™**ï¼šéš¾ä»¥è¡¨è¾¾ XOR ç­‰ç®€å•å…³ç³»

---

## 8ï¸âƒ£ å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæ³°å¦å°¼å…‹å·ç”Ÿå­˜é¢„æµ‹

```python
import pandas as pd
from sklearn.tree import DecisionTreeClassifier

# åŠ è½½æ•°æ®
titanic = pd.read_csv('titanic.csv')

# ç‰¹å¾å·¥ç¨‹
X = titanic[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]
y = titanic['Survived']

# å¤„ç†ç±»åˆ«ç‰¹å¾
X['Sex'] = X['Sex'].map({'male': 0, 'female': 1})

# è®­ç»ƒæ¨¡å‹
model = DecisionTreeClassifier(max_depth=4, min_samples_split=20)
model.fit(X, y)

# æŸ¥çœ‹ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(feature_importance)
```

### æ¡ˆä¾‹2ï¼šå®¢æˆ·æµå¤±é¢„æµ‹

```python
# åˆ†ç±»è§„åˆ™æå–
from sklearn.tree import export_text

tree_rules = export_text(model, feature_names=list(X.columns))
print(tree_rules)

# è¾“å‡ºç¤ºä¾‹:
# |--- Age <= 50.0
# |   |--- MonthlyCharges <= 70.0
# |   |   |--- class: No Churn
# |   |--- MonthlyCharges >  70.0
# |   |   |--- class: Churn
```

---

## 9ï¸âƒ£ ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”

| ç®—æ³• | å¯è§£é‡Šæ€§ | å¤„ç†éçº¿æ€§ | è¿‡æ‹Ÿåˆé£é™© | è®­ç»ƒé€Ÿåº¦ |
|------|----------|------------|------------|----------|
| å†³ç­–æ ‘ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| çº¿æ€§å›å½’ | â­â­â­â­ | â­ | â­â­ | â­â­â­â­â­ |
| é€»è¾‘å›å½’ | â­â­â­â­ | â­ | â­â­ | â­â­â­â­â­ |
| éšæœºæ£®æ— | â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­ |
| SVM | â­â­ | â­â­â­â­â­ | â­â­â­ | â­â­ |

---

## ğŸ¯ å…³é”®è¦ç‚¹

1. å†³ç­–æ ‘é€šè¿‡**é€’å½’åˆ†è£‚**æ„å»ºæ ‘å½¢å†³ç­–æ¨¡å‹
2. å¸¸ç”¨åˆ†è£‚æ ‡å‡†ï¼š**ä¿¡æ¯å¢ç›Š**ï¼ˆID3ï¼‰ã€**ä¿¡æ¯å¢ç›Šç‡**ï¼ˆC4.5ï¼‰ã€**åŸºå°¼æŒ‡æ•°**ï¼ˆCARTï¼‰
3. å®¹æ˜“**è¿‡æ‹Ÿåˆ**ï¼Œéœ€è¦é€šè¿‡**å‰ªæ**ï¼ˆé¢„å‰ªæã€åå‰ªæï¼‰æ§åˆ¶å¤æ‚åº¦
4. **å¯è§£é‡Šæ€§å¼º**ï¼Œé€‚åˆéœ€è¦ç†è§£å†³ç­–é€»è¾‘çš„åœºæ™¯
5. æ˜¯**é›†æˆå­¦ä¹ **ï¼ˆéšæœºæ£®æ—ã€GBDTã€XGBoostï¼‰çš„åŸºç¡€
6. å¯¹**æ•°æ®å˜åŒ–æ•æ„Ÿ**ï¼Œå°çš„æ‰°åŠ¨å¯èƒ½å¯¼è‡´å®Œå…¨ä¸åŒçš„æ ‘ç»“æ„

