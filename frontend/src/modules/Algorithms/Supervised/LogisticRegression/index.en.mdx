import { useState } from 'react';
import { useTranslation } from 'react-i18next';
import axios from 'axios';
import { Demo, ParameterSlider, Button, Chart, AlgorithmHeader, PlayIcon, ResetIcon } from '@common/modules/MDX/MDXComponents';
import { useLogger } from '@common/modules/Logger';
import { useExceptions, ExceptionType, ExceptionSeverity } from '@common/modules/Exceptions';

export const LogisticRegressionDemo = () => {
  const { t } = useTranslation();
  const logger = useLogger();
  const { captureException } = useExceptions();
  const [learningRate, setLearningRate] = useState(0.1);
  const [iterations, setIterations] = useState(1000);
  const [loading, setLoading] = useState(false);
  const [result, setResult] = useState(null);

  const runTraining = async () => {
    setLoading(true);
    logger.info('Started training logistic regression model', { learningRate, iterations });
    
    try {
      const response = await axios.post('/api/algorithms/supervised/logistic-regression/train', {
        learning_rate: learningRate,
        iterations: iterations,
      });
      setResult(response.data);
      logger.info('Logistic regression model trained successfully', { result: response.data });
    } catch (error) {
      logger.error('Logistic regression training failed', { error: error.message, learningRate, iterations });
      captureException(error, {
        type: ExceptionType.NETWORK,
        severity: ExceptionSeverity.HIGH,
        context: { 
          algorithm: 'logistic-regression',
          learningRate,
          iterations,
          message: 'Training failed. Please check if backend service is running'
        }
      });
    } finally {
      setLoading(false);
    }
  };

  const resetParameters = () => {
    setLearningRate(0.1);
    setIterations(1000);
    setResult(null);
  };

  return (
    <Demo title="‚ö° Interactive Experiment">
      <div className="space-y-6">
        <div className="bg-gray-50 dark:bg-gray-800 p-4 rounded-lg">
          <h4 className="font-semibold mb-4 text-gray-900 dark:text-gray-100">üìä Parameters</h4>
          
          <ParameterSlider
            label="Learning Rate"
            value={learningRate}
            onChange={setLearningRate}
            min={0.01}
            max={1.0}
            step={0.01}
          />
          
          <ParameterSlider
            label="Iterations"
            value={iterations}
            onChange={setIterations}
            min={100}
            max={5000}
            step={100}
          />

          <div className="flex gap-3 mt-6">
            <Button onClick={runTraining} icon={PlayIcon} disabled={loading}>
              {loading ? 'Training...' : 'Run Training'}
            </Button>
            <Button onClick={resetParameters} variant="secondary" icon={ResetIcon}>
              Reset
            </Button>
          </div>
        </div>

        {result && (
          <div className="space-y-4">
            <div className="bg-blue-50 dark:bg-blue-900/20 p-4 rounded-lg">
              <h4 className="font-semibold mb-2 text-gray-900 dark:text-gray-100">üìà Classification Results</h4>
              <div className="grid grid-cols-2 gap-4 text-sm">
                <div>
                  <span className="text-gray-600 dark:text-gray-400">Accuracy:</span>
                  <span className="ml-2 font-mono font-semibold">{result.accuracy?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">Precision:</span>
                  <span className="ml-2 font-mono font-semibold">{result.precision?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">Recall:</span>
                  <span className="ml-2 font-mono font-semibold">{result.recall?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">F1 Score:</span>
                  <span className="ml-2 font-mono font-semibold">{result.f1_score?.toFixed(4)}</span>
                </div>
              </div>
            </div>

            {result.plot_data && (
              <Chart
                data={result.plot_data}
                layout={{
                  title: 'Logistic Regression Decision Boundary',
                  xaxis: { title: 'Feature 1' },
                  yaxis: { title: 'Feature 2' },
                  height: 400,
                }}
              />
            )}
          </div>
        )}
      </div>
    </Demo>
  );
};

<AlgorithmHeader 
  title="Logistic Regression"
  difficulty="beginner"
  stars={2}
  time={20}
/>

## 1Ô∏è‚É£ Algorithm Overview

**Logistic Regression** is a classic **classification algorithm**. Despite having "regression" in its name, it is primarily used to solve **binary classification problems**. It maps the output of linear regression to the (0, 1) interval through the Sigmoid function, representing the probability that a sample belongs to a certain class.

### Core Concepts

- Uses **Sigmoid function** to convert linear model output into probability values
- Optimizes model parameters through **maximum likelihood estimation** or **gradient descent**
- Classifies as positive class if output probability > 0.5, otherwise negative class

### Application Scenarios

- üè• **Disease Diagnosis**: Determine illness based on symptoms and test indicators
- üìß **Spam Detection**: Identify spam emails
- üí≥ **Credit Scoring**: Assess default risk for loan applications
- üéØ **Click-Through Rate Prediction**: Predict whether users will click on ads

---

## 2Ô∏è‚É£ Historical Development

- **1838** - Belgian mathematician **Pierre Fran√ßois Verhulst** first proposed the Logistic function to describe population growth
- **1944** - **Joseph Berkson** introduced the Logistic function into statistics for binary classification
- **1958** - **David Cox** proposed the proportional hazards model, laying the foundation for modern logistic regression
- **1970s** - Logistic regression widely applied in medical statistics and epidemiology
- **21st Century** - Became a fundamental algorithm in machine learning and data mining

---

## 3Ô∏è‚É£ Mathematical Principles

### Sigmoid Function

The core of logistic regression is the **Sigmoid function** (also called Logistic function):

```math
\sigma(z) = \frac{1}{1 + e^{-z}}
```

Where $z = \mathbf{w}^T\mathbf{x} + b$ is the linear combination.

The Sigmoid function maps any real number to the (0, 1) interval with the following properties:
- $\sigma(0) = 0.5$
- $\sigma(+\infty) = 1$
- $\sigma(-\infty) = 0$
- Derivative: $\sigma'(z) = \sigma(z)(1 - \sigma(z))$

### Model Representation

For input $\mathbf{x}$, the model predicts the probability of belonging to the positive class:

```math
P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T\mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x} + b)}}
```

### Loss Function

Using **Cross-Entropy Loss**:

```math
J(\mathbf{w}, b) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h(\mathbf{x}^{(i)})) + (1-y^{(i)})\log(1-h(\mathbf{x}^{(i)}))]
```

### Gradient Descent

Parameter update rules are similar to linear regression:

```math
w_j := w_j - \alpha \frac{\partial J}{\partial w_j}
```

```math
b := b - \alpha \frac{\partial J}{\partial b}
```

---

## 4Ô∏è‚É£ Interactive Experiment ‚ö°

Binary classification using the Iris dataset:

<LogisticRegressionDemo />

---

## 5Ô∏è‚É£ Code Implementation

### Python Implementation (scikit-learn)

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load data
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data[:100, :2]  # Only first 100 samples (two classes)
y = iris.target[:100]

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create and train model
model = LogisticRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

# Evaluate
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

### Logistic Regression from Scratch

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, learning_rate=0.1, iterations=1000):
    m, n = X.shape
    w = np.zeros((n, 1))
    b = 0
    
    for i in range(iterations):
        # Forward propagation
        z = X @ w + b
        y_pred = sigmoid(z)
        
        # Calculate gradients
        dw = (1/m) * X.T @ (y_pred - y)
        db = (1/m) * np.sum(y_pred - y)
        
        # Update parameters
        w -= learning_rate * dw
        b -= learning_rate * db
        
        # Calculate loss (optional)
        if i % 100 == 0:
            loss = -(1/m) * np.sum(y * np.log(y_pred) + (1-y) * np.log(1-y_pred))
            print(f"Iteration {i}, Loss: {loss:.4f}")
    
    return w, b

def predict(X, w, b):
    z = X @ w + b
    y_pred = sigmoid(z)
    return (y_pred >= 0.5).astype(int)
```

---

## 6Ô∏è‚É£ Multi-class Extension

Logistic regression can be extended to multi-class problems:

### One-vs-Rest (OvR)

Train a binary classifier for each class, select the class with highest probability:

```python
from sklearn.linear_model import LogisticRegression

# multi_class='ovr' for One-vs-Rest strategy
model = LogisticRegression(multi_class='ovr')
model.fit(X_train, y_train)
```

### Softmax Regression (Multinomial Logistic Regression)

Use Softmax function to directly output probability distribution over multiple classes:

```math
P(y=k|\mathbf{x}) = \frac{e^{\mathbf{w}_k^T\mathbf{x}}}{\sum_{j=1}^{K}e^{\mathbf{w}_j^T\mathbf{x}}}
```

```python
model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
model.fit(X_train, y_train)
```

---

## 7Ô∏è‚É£ Pros and Cons

### ‚úÖ Advantages

- **Probability Output**: Provides not only classification results but also probabilities for uncertainty assessment
- **Simple and Efficient**: Low computational complexity, suitable for large-scale data
- **Highly Interpretable**: Coefficients reflect feature impact on classification (positive or negative correlation)
- **Regularization Support**: Easy to add L1/L2 regularization to prevent overfitting

### ‚ùå Disadvantages

- **Linear Decision Boundary**: Can only handle linearly separable or approximately linearly separable problems
- **Feature Engineering Dependent**: Requires manual feature construction to capture nonlinear relationships
- **Sensitive to Outliers**: Extreme values can affect model performance
- **Class Imbalance**: Poor performance on imbalanced datasets

---

## 8Ô∏è‚É£ Real-world Applications

### Case 1: Credit Card Fraud Detection

```python
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

# Handle class imbalance (using class_weight)
model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(X_scaled, y_train)

# Predict probabilities
y_proba = model.predict_proba(X_test)[:, 1]

# Adjust threshold (increase recall)
threshold = 0.3
y_pred_custom = (y_proba >= threshold).astype(int)
```

### Case 2: Medical Diagnosis

```python
# Use L1 regularization for feature selection
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)
model.fit(X_train, y_train)

# View important features
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'coefficient': model.coef_[0]
}).sort_values('coefficient', key=abs, ascending=False)

print("Important Features:")
print(feature_importance[feature_importance.coefficient != 0])
```

---

## 9Ô∏è‚É£ Regularization

To prevent overfitting, regularization terms are usually added:

### L2 Regularization (Ridge)

```math
J(\mathbf{w}) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h(\mathbf{x}^{(i)})) + (1-y^{(i)})\log(1-h(\mathbf{x}^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2
```

### L1 Regularization (Lasso)

```math
J(\mathbf{w}) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h(\mathbf{x}^{(i)})) + (1-y^{(i)})\log(1-h(\mathbf{x}^{(i)}))] + \frac{\lambda}{m}\sum_{j=1}^{n}|w_j|
```

```python
# L2 regularization (default)
model_l2 = LogisticRegression(penalty='l2', C=1.0)

# L1 regularization
model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)

# Note: C is the inverse of regularization strength; smaller C means stronger regularization
```

---

## üéØ Key Takeaways

1. Logistic regression is a **classification algorithm** that outputs probabilities through the Sigmoid function
2. Uses **cross-entropy loss** and **gradient descent** to optimize parameters
3. Output represents the **probability** of belonging to the positive class, typically using 0.5 as threshold
4. Can be extended to multi-class through **One-vs-Rest** or **Softmax**
5. Supports **L1/L2 regularization** to prevent overfitting; L1 also enables feature selection
6. Suitable for **linearly separable** or **approximately linearly separable** binary classification problems

