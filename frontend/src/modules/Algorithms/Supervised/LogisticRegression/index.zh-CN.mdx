import { useState } from 'react';
import { useTranslation } from 'react-i18next';
import axios from 'axios';
import { Demo, ParameterSlider, Button, Chart, AlgorithmHeader, PlayIcon, ResetIcon } from '@common/modules/MDX/MDXComponents';
import { useLogger } from '@common/modules/Logger';
import { useExceptions, ExceptionType, ExceptionSeverity } from '@common/modules/Exceptions';

export const LogisticRegressionDemo = () => {
  const { t } = useTranslation();
  const logger = useLogger();
  const { captureException } = useExceptions();
  const [learningRate, setLearningRate] = useState(0.1);
  const [iterations, setIterations] = useState(1000);
  const [loading, setLoading] = useState(false);
  const [result, setResult] = useState(null);

  const runTraining = async () => {
    setLoading(true);
    logger.info('å¼€å§‹è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹', { learningRate, iterations });
    
    try {
      const response = await axios.post('/api/algorithms/supervised/logistic-regression/train', {
        learning_rate: learningRate,
        iterations: iterations,
      });
      setResult(response.data);
      logger.info('é€»è¾‘å›å½’æ¨¡å‹è®­ç»ƒæˆåŠŸ', { result: response.data });
    } catch (error) {
      logger.error('é€»è¾‘å›å½’æ¨¡å‹è®­ç»ƒå¤±è´¥', { error: error.message, learningRate, iterations });
      captureException(error, {
        type: ExceptionType.NETWORK,
        severity: ExceptionSeverity.HIGH,
        context: { 
          algorithm: 'logistic-regression',
          learningRate,
          iterations,
          message: 'è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥åç«¯æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ'
        }
      });
    } finally {
      setLoading(false);
    }
  };

  const resetParameters = () => {
    setLearningRate(0.1);
    setIterations(1000);
    setResult(null);
  };

  return (
    <Demo title="âš¡ äº¤äº’å¼å®éªŒ">
      <div className="space-y-6">
        <div className="bg-gray-50 dark:bg-gray-800 p-4 rounded-lg">
          <h4 className="font-semibold mb-4 text-gray-900 dark:text-gray-100">ğŸ“Š å‚æ•°è°ƒæ•´</h4>
          
          <ParameterSlider
            label="å­¦ä¹ ç‡ (Learning Rate)"
            value={learningRate}
            onChange={setLearningRate}
            min={0.01}
            max={1.0}
            step={0.01}
          />
          
          <ParameterSlider
            label="è¿­ä»£æ¬¡æ•° (Iterations)"
            value={iterations}
            onChange={setIterations}
            min={100}
            max={5000}
            step={100}
          />

          <div className="flex gap-3 mt-6">
            <Button onClick={runTraining} icon={PlayIcon} disabled={loading}>
              {loading ? 'è®­ç»ƒä¸­...' : 'è¿è¡Œè®­ç»ƒ'}
            </Button>
            <Button onClick={resetParameters} variant="secondary" icon={ResetIcon}>
              é‡ç½®å‚æ•°
            </Button>
          </div>
        </div>

        {result && (
          <div className="space-y-4">
            <div className="bg-blue-50 dark:bg-blue-900/20 p-4 rounded-lg">
              <h4 className="font-semibold mb-2 text-gray-900 dark:text-gray-100">ğŸ“ˆ åˆ†ç±»ç»“æœ</h4>
              <div className="grid grid-cols-2 gap-4 text-sm">
                <div>
                  <span className="text-gray-600 dark:text-gray-400">å‡†ç¡®ç‡ (Accuracy):</span>
                  <span className="ml-2 font-mono font-semibold">{result.accuracy?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">ç²¾ç¡®ç‡ (Precision):</span>
                  <span className="ml-2 font-mono font-semibold">{result.precision?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">å¬å›ç‡ (Recall):</span>
                  <span className="ml-2 font-mono font-semibold">{result.recall?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">F1 Score:</span>
                  <span className="ml-2 font-mono font-semibold">{result.f1_score?.toFixed(4)}</span>
                </div>
              </div>
            </div>

            {result.plot_data && (
              <Chart
                data={result.plot_data}
                layout={{
                  title: 'é€»è¾‘å›å½’å†³ç­–è¾¹ç•Œ',
                  xaxis: { title: 'Feature 1' },
                  yaxis: { title: 'Feature 2' },
                  height: 400,
                }}
              />
            )}
          </div>
        )}
      </div>
    </Demo>
  );
};

<AlgorithmHeader 
  title="é€»è¾‘å›å½’ Logistic Regression"
  difficulty="beginner"
  stars={2}
  time={20}
/>

## 1ï¸âƒ£ ç®—æ³•æ¦‚è§ˆ

**é€»è¾‘å›å½’**ï¼ˆLogistic Regressionï¼‰æ˜¯ä¸€ç§ç»å…¸çš„**åˆ†ç±»ç®—æ³•**ï¼Œå°½ç®¡åå­—ä¸­å¸¦æœ‰"å›å½’"ï¼Œä½†å®ƒä¸»è¦ç”¨äºè§£å†³**äºŒåˆ†ç±»é—®é¢˜**ã€‚é€šè¿‡ Sigmoid å‡½æ•°å°†çº¿æ€§å›å½’çš„è¾“å‡ºæ˜ å°„åˆ° (0, 1) åŒºé—´ï¼Œè¡¨ç¤ºæ ·æœ¬å±äºæŸä¸€ç±»åˆ«çš„æ¦‚ç‡ã€‚

### æ ¸å¿ƒæ€æƒ³

- ä½¿ç”¨ **Sigmoid å‡½æ•°**å°†çº¿æ€§æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡å€¼
- é€šè¿‡**æœ€å¤§ä¼¼ç„¶ä¼°è®¡**æˆ–**æ¢¯åº¦ä¸‹é™**ä¼˜åŒ–æ¨¡å‹å‚æ•°
- è¾“å‡ºæ¦‚ç‡å¤§äº 0.5 åˆ¤å®šä¸ºæ­£ç±»ï¼Œå¦åˆ™ä¸ºè´Ÿç±»

### åº”ç”¨åœºæ™¯

- ğŸ¥ **ç–¾ç—…è¯Šæ–­**ï¼šæ ¹æ®ç—‡çŠ¶å’Œæ£€æŸ¥æŒ‡æ ‡åˆ¤æ–­æ˜¯å¦æ‚£ç—…
- ğŸ“§ **åƒåœ¾é‚®ä»¶æ£€æµ‹**ï¼šåˆ¤æ–­é‚®ä»¶æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶
- ğŸ’³ **ä¿¡ç”¨è¯„åˆ†**ï¼šè¯„ä¼°è´·æ¬¾ç”³è¯·çš„è¿çº¦é£é™©
- ğŸ¯ **ç‚¹å‡»ç‡é¢„æµ‹**ï¼šé¢„æµ‹ç”¨æˆ·æ˜¯å¦ä¼šç‚¹å‡»å¹¿å‘Š

---

## 2ï¸âƒ£ å‘å±•å†å²

- **1838å¹´** - æ¯”åˆ©æ—¶æ•°å­¦å®¶**Pierre FranÃ§ois Verhulst**é¦–æ¬¡æå‡º Logistic å‡½æ•°æè¿°äººå£å¢é•¿
- **1944å¹´** - **Joseph Berkson**å°† Logistic å‡½æ•°å¼•å…¥ç»Ÿè®¡å­¦ï¼Œç”¨äºäºŒåˆ†ç±»é—®é¢˜
- **1958å¹´** - **David Cox**æå‡ºæ¯”ä¾‹é£é™©æ¨¡å‹ï¼Œå¥ å®šäº†ç°ä»£é€»è¾‘å›å½’çš„åŸºç¡€
- **1970å¹´ä»£** - é€»è¾‘å›å½’åœ¨åŒ»å­¦ç»Ÿè®¡å’Œæµè¡Œç—…å­¦ä¸­å¹¿æ³›åº”ç”¨
- **21ä¸–çºª** - æˆä¸ºæœºå™¨å­¦ä¹ å’Œæ•°æ®æŒ–æ˜çš„åŸºç¡€ç®—æ³•

---

## 3ï¸âƒ£ æ•°å­¦åŸç†

### Sigmoid å‡½æ•°

é€»è¾‘å›å½’çš„æ ¸å¿ƒæ˜¯ **Sigmoid å‡½æ•°**ï¼ˆä¹Ÿç§°ä¸º Logistic å‡½æ•°ï¼‰ï¼š

```math
\sigma(z) = \frac{1}{1 + e^{-z}}
```

å…¶ä¸­ $z = \mathbf{w}^T\mathbf{x} + b$ æ˜¯çº¿æ€§ç»„åˆã€‚

Sigmoid å‡½æ•°å°†ä»»æ„å®æ•°æ˜ å°„åˆ° (0, 1) åŒºé—´ï¼Œå…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼š
- $\sigma(0) = 0.5$
- $\sigma(+\infty) = 1$
- $\sigma(-\infty) = 0$
- å¯¼æ•°ï¼š$\sigma'(z) = \sigma(z)(1 - \sigma(z))$

### æ¨¡å‹è¡¨ç¤º

å¯¹äºè¾“å…¥ $\mathbf{x}$ï¼Œæ¨¡å‹é¢„æµ‹å…¶å±äºæ­£ç±»çš„æ¦‚ç‡ï¼š

```math
P(y=1|\mathbf{x}) = \sigma(\mathbf{w}^T\mathbf{x} + b) = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x} + b)}}
```

### æŸå¤±å‡½æ•°

ä½¿ç”¨**äº¤å‰ç†µæŸå¤±**ï¼ˆCross-Entropy Lossï¼‰ï¼š

```math
J(\mathbf{w}, b) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h(\mathbf{x}^{(i)})) + (1-y^{(i)})\log(1-h(\mathbf{x}^{(i)}))]
```

### æ¢¯åº¦ä¸‹é™

å‚æ•°æ›´æ–°è§„åˆ™ä¸çº¿æ€§å›å½’ç±»ä¼¼ï¼š

```math
w_j := w_j - \alpha \frac{\partial J}{\partial w_j}
```

```math
b := b - \alpha \frac{\partial J}{\partial b}
```

---

## 4ï¸âƒ£ äº¤äº’å¼å®éªŒ âš¡

åœ¨ä¸‹é¢çš„æ¼”ç¤ºä¸­ï¼Œä½¿ç”¨é¸¢å°¾èŠ±æ•°æ®é›†è¿›è¡ŒäºŒåˆ†ç±»ï¼š

<LogisticRegressionDemo />

---

## 5ï¸âƒ£ ä»£ç å®ç°

### Python å®ç°ï¼ˆscikit-learnï¼‰

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# åŠ è½½æ•°æ®
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data[:100, :2]  # åªå–å‰100ä¸ªæ ·æœ¬ï¼ˆä¸¤ç±»ï¼‰
y = iris.target[:100]

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
model = LogisticRegression()
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)

# è¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))
print("\næ··æ·†çŸ©é˜µ:")
print(confusion_matrix(y_test, y_pred))
```

### ä»é›¶å®ç°é€»è¾‘å›å½’

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, learning_rate=0.1, iterations=1000):
    m, n = X.shape
    w = np.zeros((n, 1))
    b = 0
    
    for i in range(iterations):
        # å‰å‘ä¼ æ’­
        z = X @ w + b
        y_pred = sigmoid(z)
        
        # è®¡ç®—æ¢¯åº¦
        dw = (1/m) * X.T @ (y_pred - y)
        db = (1/m) * np.sum(y_pred - y)
        
        # æ›´æ–°å‚æ•°
        w -= learning_rate * dw
        b -= learning_rate * db
        
        # è®¡ç®—æŸå¤±ï¼ˆå¯é€‰ï¼‰
        if i % 100 == 0:
            loss = -(1/m) * np.sum(y * np.log(y_pred) + (1-y) * np.log(1-y_pred))
            print(f"Iteration {i}, Loss: {loss:.4f}")
    
    return w, b

def predict(X, w, b):
    z = X @ w + b
    y_pred = sigmoid(z)
    return (y_pred >= 0.5).astype(int)
```

---

## 6ï¸âƒ£ å¤šåˆ†ç±»æ‰©å±•

é€»è¾‘å›å½’å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ‰©å±•åˆ°å¤šåˆ†ç±»é—®é¢˜ï¼š

### One-vs-Rest (OvR)

ä¸ºæ¯ä¸ªç±»åˆ«è®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»å™¨ï¼Œé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ï¼š

```python
from sklearn.linear_model import LogisticRegression

# multi_class='ovr' è¡¨ç¤º One-vs-Rest ç­–ç•¥
model = LogisticRegression(multi_class='ovr')
model.fit(X_train, y_train)
```

### Softmax å›å½’ï¼ˆå¤šé¡¹é€»è¾‘å›å½’ï¼‰

ä½¿ç”¨ Softmax å‡½æ•°ç›´æ¥è¾“å‡ºå¤šä¸ªç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒï¼š

```math
P(y=k|\mathbf{x}) = \frac{e^{\mathbf{w}_k^T\mathbf{x}}}{\sum_{j=1}^{K}e^{\mathbf{w}_j^T\mathbf{x}}}
```

```python
model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
model.fit(X_train, y_train)
```

---

## 7ï¸âƒ£ ä¼˜ç¼ºç‚¹åˆ†æ

### âœ… ä¼˜ç‚¹

- **æ¦‚ç‡è¾“å‡º**ï¼šä¸ä»…ç»™å‡ºåˆ†ç±»ç»“æœï¼Œè¿˜èƒ½è¾“å‡ºæ¦‚ç‡ï¼Œä¾¿äºè¯„ä¼°ä¸ç¡®å®šæ€§
- **ç®€å•é«˜æ•ˆ**ï¼šè®¡ç®—å¤æ‚åº¦ä½ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®
- **å¯è§£é‡Šæ€§å¼º**ï¼šç³»æ•°åæ˜ ç‰¹å¾å¯¹åˆ†ç±»çš„å½±å“ï¼ˆæ­£ç›¸å…³æˆ–è´Ÿç›¸å…³ï¼‰
- **æ­£åˆ™åŒ–æ”¯æŒ**ï¼šæ˜“äºæ·»åŠ  L1/L2 æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ

### âŒ ç¼ºç‚¹

- **çº¿æ€§å†³ç­–è¾¹ç•Œ**ï¼šåªèƒ½å¤„ç†çº¿æ€§å¯åˆ†æˆ–è¿‘ä¼¼çº¿æ€§å¯åˆ†çš„é—®é¢˜
- **ç‰¹å¾å·¥ç¨‹ä¾èµ–**ï¼šéœ€è¦æ‰‹åŠ¨æ„é€ ç‰¹å¾æ¥æ•æ‰éçº¿æ€§å…³ç³»
- **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ**ï¼šæç«¯å€¼å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½
- **ç±»åˆ«ä¸å¹³è¡¡**ï¼šåœ¨ä¸å¹³è¡¡æ•°æ®é›†ä¸Šè¡¨ç°è¾ƒå·®

---

## 8ï¸âƒ£ å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹

```python
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

# æ ‡å‡†åŒ–ç‰¹å¾
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)

# å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼ˆä½¿ç”¨ class_weightï¼‰
model = LogisticRegression(class_weight='balanced', max_iter=1000)
model.fit(X_scaled, y_train)

# é¢„æµ‹æ¦‚ç‡
y_proba = model.predict_proba(X_test)[:, 1]

# è°ƒæ•´é˜ˆå€¼ï¼ˆæé«˜å¬å›ç‡ï¼‰
threshold = 0.3
y_pred_custom = (y_proba >= threshold).astype(int)
```

### æ¡ˆä¾‹2ï¼šåŒ»ç–—è¯Šæ–­

```python
# ä½¿ç”¨ L1 æ­£åˆ™åŒ–è¿›è¡Œç‰¹å¾é€‰æ‹©
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)
model.fit(X_train, y_train)

# æŸ¥çœ‹é‡è¦ç‰¹å¾
feature_importance = pd.DataFrame({
    'feature': feature_names,
    'coefficient': model.coef_[0]
}).sort_values('coefficient', key=abs, ascending=False)

print("é‡è¦ç‰¹å¾:")
print(feature_importance[feature_importance.coefficient != 0])
```

---

## 9ï¸âƒ£ æ­£åˆ™åŒ–

ä¸ºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé€šå¸¸æ·»åŠ æ­£åˆ™åŒ–é¡¹ï¼š

### L2 æ­£åˆ™åŒ–ï¼ˆRidgeï¼‰

```math
J(\mathbf{w}) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h(\mathbf{x}^{(i)})) + (1-y^{(i)})\log(1-h(\mathbf{x}^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2
```

### L1 æ­£åˆ™åŒ–ï¼ˆLassoï¼‰

```math
J(\mathbf{w}) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h(\mathbf{x}^{(i)})) + (1-y^{(i)})\log(1-h(\mathbf{x}^{(i)}))] + \frac{\lambda}{m}\sum_{j=1}^{n}|w_j|
```

```python
# L2 æ­£åˆ™åŒ–ï¼ˆé»˜è®¤ï¼‰
model_l2 = LogisticRegression(penalty='l2', C=1.0)

# L1 æ­£åˆ™åŒ–
model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)

# æ³¨æ„ï¼šC æ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„å€’æ•°ï¼ŒC è¶Šå°ï¼Œæ­£åˆ™åŒ–è¶Šå¼º
```

---

## ğŸ¯ å…³é”®è¦ç‚¹

1. é€»è¾‘å›å½’æ˜¯**åˆ†ç±»ç®—æ³•**ï¼Œé€šè¿‡ Sigmoid å‡½æ•°è¾“å‡ºæ¦‚ç‡
2. ä½¿ç”¨**äº¤å‰ç†µæŸå¤±**å’Œ**æ¢¯åº¦ä¸‹é™**ä¼˜åŒ–å‚æ•°
3. è¾“å‡ºå€¼è¡¨ç¤ºæ ·æœ¬å±äºæ­£ç±»çš„**æ¦‚ç‡**ï¼Œé€šå¸¸ä»¥ 0.5 ä¸ºé˜ˆå€¼
4. å¯ä»¥é€šè¿‡ **One-vs-Rest** æˆ– **Softmax** æ‰©å±•åˆ°å¤šåˆ†ç±»
5. æ”¯æŒ **L1/L2 æ­£åˆ™åŒ–**é˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒL1 è¿˜èƒ½å®ç°ç‰¹å¾é€‰æ‹©
6. é€‚ç”¨äº**çº¿æ€§å¯åˆ†**æˆ–**è¿‘ä¼¼çº¿æ€§å¯åˆ†**çš„äºŒåˆ†ç±»é—®é¢˜

