import { useState } from 'react';
import { useTranslation } from 'react-i18next';
import axios from 'axios';
import { Demo, ParameterSlider, Button, Chart, AlgorithmHeader, PlayIcon, ResetIcon } from '@common/modules/MDX/MDXComponents';
import { useLogger } from '@common/modules/Logger';
import { useExceptions, ExceptionType, ExceptionSeverity } from '@common/modules/Exceptions';

export const RandomForestDemo = () => {
  const { t } = useTranslation();
  const logger = useLogger();
  const { captureException } = useExceptions();
  const [nEstimators, setNEstimators] = useState(100);
  const [maxDepth, setMaxDepth] = useState(10);
  const [loading, setLoading] = useState(false);
  const [result, setResult] = useState(null);

  const runTraining = async () => {
    setLoading(true);
    logger.info('å¼€å§‹è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹', { nEstimators, maxDepth });
    
    try {
      const response = await axios.post('/api/algorithms/supervised/random-forest/train', {
        n_estimators: nEstimators,
        max_depth: maxDepth,
      });
      setResult(response.data);
      logger.info('éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒæˆåŠŸ', { result: response.data });
    } catch (error) {
      logger.error('éšæœºæ£®æ—æ¨¡å‹è®­ç»ƒå¤±è´¥', { error: error.message, nEstimators, maxDepth });
      captureException(error, {
        type: ExceptionType.NETWORK,
        severity: ExceptionSeverity.HIGH,
        context: { 
          algorithm: 'random-forest',
          nEstimators,
          maxDepth,
          message: 'è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥åç«¯æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ'
        }
      });
    } finally {
      setLoading(false);
    }
  };

  const resetParameters = () => {
    setNEstimators(100);
    setMaxDepth(10);
    setResult(null);
  };

  return (
    <Demo title="âš¡ äº¤äº’å¼å®éªŒ">
      <div className="space-y-6">
        <div className="bg-gray-50 dark:bg-gray-800 p-4 rounded-lg">
          <h4 className="font-semibold mb-4 text-gray-900 dark:text-gray-100">ğŸ“Š å‚æ•°è°ƒæ•´</h4>
          
          <ParameterSlider
            label="å†³ç­–æ ‘æ•°é‡ (N Estimators)"
            value={nEstimators}
            onChange={setNEstimators}
            min={10}
            max={500}
            step={10}
          />
          
          <ParameterSlider
            label="æœ€å¤§æ·±åº¦ (Max Depth)"
            value={maxDepth}
            onChange={setMaxDepth}
            min={1}
            max={30}
            step={1}
          />

          <div className="flex gap-3 mt-6">
            <Button onClick={runTraining} icon={PlayIcon} disabled={loading}>
              {loading ? 'è®­ç»ƒä¸­...' : 'è¿è¡Œè®­ç»ƒ'}
            </Button>
            <Button onClick={resetParameters} variant="secondary" icon={ResetIcon}>
              é‡ç½®å‚æ•°
            </Button>
          </div>
        </div>

        {result && (
          <div className="space-y-4">
            <div className="bg-green-50 dark:bg-green-900/20 p-4 rounded-lg">
              <h4 className="font-semibold mb-2 text-gray-900 dark:text-gray-100">ğŸŒ² è®­ç»ƒç»“æœ</h4>
              <div className="grid grid-cols-2 gap-4 text-sm">
                <div>
                  <span className="text-gray-600 dark:text-gray-400">å‡†ç¡®ç‡ (Accuracy):</span>
                  <span className="ml-2 font-mono font-semibold">{result.accuracy?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">F1 Score:</span>
                  <span className="ml-2 font-mono font-semibold">{result.f1_score?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">è®­ç»ƒæ—¶é—´:</span>
                  <span className="ml-2 font-mono font-semibold">{result.training_time?.toFixed(3)}s</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">æ ‘çš„æ•°é‡:</span>
                  <span className="ml-2 font-mono font-semibold">{result.n_trees}</span>
                </div>
              </div>
            </div>

            {result.feature_importance && (
              <Chart
                data={result.feature_importance}
                layout={{
                  title: 'ç‰¹å¾é‡è¦æ€§',
                  xaxis: { title: 'é‡è¦æ€§' },
                  yaxis: { title: 'ç‰¹å¾' },
                  height: 400,
                }}
              />
            )}
          </div>
        )}
      </div>
    </Demo>
  );
};

<AlgorithmHeader 
  title="éšæœºæ£®æ— Random Forest"
  difficulty="intermediate"
  stars={3}
  time={30}
/>

## 1ï¸âƒ£ ç®—æ³•æ¦‚è§ˆ

**éšæœºæ£®æ—**ï¼ˆRandom Forestï¼‰æ˜¯ä¸€ç§åŸºäº**é›†æˆå­¦ä¹ **ï¼ˆEnsemble Learningï¼‰çš„å¼ºå¤§ç®—æ³•ï¼Œé€šè¿‡æ„å»ºå¤šæ£µå†³ç­–æ ‘å¹¶**æŠ•ç¥¨è¡¨å†³**æ¥æé«˜é¢„æµ‹æ€§èƒ½ã€‚å®ƒç»“åˆäº†**Bagging**ï¼ˆBootstrap Aggregatingï¼‰å’Œ**éšæœºç‰¹å¾é€‰æ‹©**ä¸¤å¤§æŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½äº†å•æ£µå†³ç­–æ ‘çš„è¿‡æ‹Ÿåˆé£é™©ã€‚

### æ ¸å¿ƒæ€æƒ³

- ä»è®­ç»ƒé›†ä¸­**éšæœºæŠ½æ ·**ï¼ˆæœ‰æ”¾å›æŠ½æ ·ï¼ŒBootstrapï¼‰æ„å»ºå¤šä¸ªå­é›†
- åœ¨æ¯ä¸ªå­é›†ä¸Šè®­ç»ƒä¸€æ£µå†³ç­–æ ‘
- æ¯æ¬¡åˆ†è£‚æ—¶åªè€ƒè™‘**éšæœºé€‰æ‹©çš„ç‰¹å¾å­é›†**
- **å¤šæ•°æŠ•ç¥¨**ï¼ˆåˆ†ç±»ï¼‰æˆ–**å¹³å‡å€¼**ï¼ˆå›å½’ï¼‰ä½œä¸ºæœ€ç»ˆé¢„æµ‹

### åº”ç”¨åœºæ™¯

- ğŸ¥ **ç–¾ç—…è¯Šæ–­**ï¼šç»¼åˆå¤šä¸ªç‰¹å¾è¿›è¡Œç–¾ç—…é¢„æµ‹
- ğŸ’³ **ä¿¡ç”¨è¯„åˆ†**ï¼šè¯„ä¼°è´·æ¬¾è¿çº¦é£é™©
- ğŸ“Š **è‚¡ç¥¨é¢„æµ‹**ï¼šé¢„æµ‹è‚¡ç¥¨ä»·æ ¼èµ°åŠ¿
- ğŸ¯ **æ¨èç³»ç»Ÿ**ï¼šç”¨æˆ·è¡Œä¸ºé¢„æµ‹å’Œå†…å®¹æ¨è

---

## 2ï¸âƒ£ å‘å±•å†å²

- **1994å¹´** - **Tin Kam Ho** æå‡ºéšæœºå­ç©ºé—´æ–¹æ³•ï¼ˆRandom Subspace Methodï¼‰
- **1996å¹´** - **Leo Breiman** æå‡º **Bagging** ç®—æ³•ï¼ˆBootstrap Aggregatingï¼‰
- **2001å¹´** - **Leo Breiman** æ­£å¼æå‡º **Random Forest** ç®—æ³•ï¼Œç»“åˆ Bagging å’Œéšæœºç‰¹å¾é€‰æ‹©
- **2003å¹´** - Breiman å’Œ Cutler å®Œå–„éšæœºæ£®æ—ç†è®ºï¼Œå¹¶æ¨å¹¿åº”ç”¨
- **21ä¸–çºª** - éšæœºæ£®æ—æˆä¸º Kaggle ç«èµ›å’Œå·¥ä¸šç•Œæœ€å—æ¬¢è¿çš„ç®—æ³•ä¹‹ä¸€

---

## 3ï¸âƒ£ æ•°å­¦åŸç†

### Baggingï¼ˆBootstrap Aggregatingï¼‰

ä»åŸå§‹æ•°æ®é›† $D$ ä¸­è¿›è¡Œ $B$ æ¬¡**æœ‰æ”¾å›éšæœºæŠ½æ ·**ï¼Œç”Ÿæˆ $B$ ä¸ªå­æ•°æ®é›† $D_1, D_2, ..., D_B$ï¼š

```math
D_b = \{(\mathbf{x}_i, y_i)\}, \quad i \sim Uniform(1, N)
```

åœ¨æ¯ä¸ªå­é›†ä¸Šè®­ç»ƒä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼ˆå†³ç­–æ ‘ï¼‰$h_b$ã€‚

### éšæœºç‰¹å¾é€‰æ‹©

åœ¨æ¯æ¬¡èŠ‚ç‚¹åˆ†è£‚æ—¶ï¼Œä¸è€ƒè™‘æ‰€æœ‰ $d$ ä¸ªç‰¹å¾ï¼Œè€Œæ˜¯éšæœºé€‰æ‹© $m$ ä¸ªç‰¹å¾ï¼ˆ$m \ll d$ï¼‰ï¼š

```math
m = \lfloor\sqrt{d}\rfloor \quad \text{(åˆ†ç±»ä»»åŠ¡)}
```

```math
m = \lfloor d/3 \rfloor \quad \text{(å›å½’ä»»åŠ¡)}
```

### é›†æˆé¢„æµ‹

**åˆ†ç±»ä»»åŠ¡**ï¼šå¤šæ•°æŠ•ç¥¨

```math
H(\mathbf{x}) = \arg\max_{y}\sum_{b=1}^{B}I(h_b(\mathbf{x}) = y)
```

**å›å½’ä»»åŠ¡**ï¼šå¹³å‡å€¼

```math
H(\mathbf{x}) = \frac{1}{B}\sum_{b=1}^{B}h_b(\mathbf{x})
```

### è¢‹å¤–è¯¯å·®ï¼ˆOut-of-Bag Errorï¼‰

æ¯ä¸ªæ ·æœ¬å¹³å‡åªå‡ºç°åœ¨çº¦ **63.2%** çš„ Bootstrap æ ·æœ¬ä¸­ï¼ˆ$1 - e^{-1} \approx 0.632$ï¼‰ï¼Œå‰©ä½™ **36.8%** çš„æ ·æœ¬å¯ä½œä¸ºéªŒè¯é›†ï¼š

```math
OOB\_Error = \frac{1}{N}\sum_{i=1}^{N}I(H_{OOB}^{(i)}(\mathbf{x}_i) \neq y_i)
```

---

## 4ï¸âƒ£ äº¤äº’å¼å®éªŒ âš¡

è°ƒæ•´éšæœºæ£®æ—çš„å…³é”®å‚æ•°ï¼Œè§‚å¯Ÿå¯¹æ€§èƒ½å’Œè®­ç»ƒæ—¶é—´çš„å½±å“ï¼š

<RandomForestDemo />

---

## 5ï¸âƒ£ ä»£ç å®ç°

### Python å®ç°ï¼ˆscikit-learnï¼‰

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
import pandas as pd

# åŠ è½½æ•°æ®
from sklearn.datasets import load_iris
iris = load_iris()
X, y = iris.data, iris.target

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ›å»ºéšæœºæ£®æ—åˆ†ç±»å™¨
model = RandomForestClassifier(
    n_estimators=100,        # å†³ç­–æ ‘æ•°é‡
    max_depth=10,            # æœ€å¤§æ·±åº¦
    min_samples_split=2,     # åˆ†è£‚æ‰€éœ€æœ€å°æ ·æœ¬æ•°
    min_samples_leaf=1,      # å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
    max_features='sqrt',     # æ¯æ¬¡åˆ†è£‚è€ƒè™‘çš„ç‰¹å¾æ•°
    bootstrap=True,          # æ˜¯å¦ä½¿ç”¨ Bootstrap æŠ½æ ·
    oob_score=True,          # æ˜¯å¦è®¡ç®—è¢‹å¤–è¯¯å·®
    n_jobs=-1,               # å¹¶è¡Œè®¡ç®—
    random_state=42
)

# è®­ç»ƒæ¨¡å‹
model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
print(f"è¢‹å¤–è¯¯å·®: {1 - model.oob_score_:.4f}")

# ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)
print("\nç‰¹å¾é‡è¦æ€§:")
print(feature_importance)
```

### éšæœºæ£®æ—å›å½’

```python
from sklearn.ensemble import RandomForestRegressor

# å›å½’ä»»åŠ¡
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42
)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

---

## 6ï¸âƒ£ ç‰¹å¾é‡è¦æ€§åˆ†æ

éšæœºæ£®æ—èƒ½å¤Ÿè®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼Œå¸®åŠ©ç†è§£å“ªäº›ç‰¹å¾å¯¹é¢„æµ‹æœ€æœ‰å½±å“ï¼š

### åŸºäºä¸çº¯åº¦å‡å°‘ï¼ˆGini Importanceï¼‰

```math
Importance(X_j) = \frac{1}{B}\sum_{b=1}^{B}\sum_{t \in T_b}I(v_t = X_j) \cdot \Delta Gini_t
```

å…¶ä¸­ $\Delta Gini_t$ æ˜¯èŠ‚ç‚¹ $t$ åˆ†è£‚ååŸºå°¼æŒ‡æ•°çš„å‡å°‘é‡ã€‚

### Python å®ç°

```python
import matplotlib.pyplot as plt

# è·å–ç‰¹å¾é‡è¦æ€§
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.title("ç‰¹å¾é‡è¦æ€§")
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), [iris.feature_names[i] for i in indices], rotation=45)
plt.tight_layout()
plt.show()
```

### åŸºäºæ’åˆ—é‡è¦æ€§ï¼ˆPermutation Importanceï¼‰

```python
from sklearn.inspection import permutation_importance

# è®¡ç®—æ’åˆ—é‡è¦æ€§
perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)

print("æ’åˆ—é‡è¦æ€§:")
for i in perm_importance.importances_mean.argsort()[::-1]:
    print(f"{iris.feature_names[i]}: {perm_importance.importances_mean[i]:.4f} +/- {perm_importance.importances_std[i]:.4f}")
```

---

## 7ï¸âƒ£ è¶…å‚æ•°è°ƒä¼˜

### ç½‘æ ¼æœç´¢

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("æœ€ä½³å‚æ•°:", grid_search.best_params_)
print("æœ€ä½³å¾—åˆ†:", grid_search.best_score_)
```

### éšæœºæœç´¢

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(5, 30),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', None]
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_dist,
    n_iter=100,
    cv=5,
    random_state=42,
    n_jobs=-1
)

random_search.fit(X_train, y_train)
```

---

## 8ï¸âƒ£ ä¼˜ç¼ºç‚¹åˆ†æ

### âœ… ä¼˜ç‚¹

- **é«˜ç²¾åº¦**ï¼šé›†æˆå¤šä¸ªå†³ç­–æ ‘ï¼Œæ˜¾è‘—æå‡é¢„æµ‹æ€§èƒ½
- **æŠ—è¿‡æ‹Ÿåˆ**ï¼šé€šè¿‡éšæœºæ€§å’Œé›†æˆé™ä½æ–¹å·®
- **å¤„ç†é«˜ç»´æ•°æ®**ï¼šèƒ½æœ‰æ•ˆå¤„ç†å¤§é‡ç‰¹å¾
- **ç‰¹å¾é‡è¦æ€§**ï¼šè‡ªåŠ¨è¯„ä¼°ç‰¹å¾å¯¹é¢„æµ‹çš„è´¡çŒ®
- **é²æ£’æ€§å¼º**ï¼šå¯¹ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼ä¸æ•æ„Ÿ
- **å¹¶è¡ŒåŒ–**ï¼šæ ‘ä¹‹é—´ç‹¬ç«‹ï¼Œå¯å¹¶è¡Œè®­ç»ƒ

### âŒ ç¼ºç‚¹

- **æ¨¡å‹å¤æ‚**ï¼šåŒ…å«å¤šæ£µæ ‘ï¼Œæ¨¡å‹ä½“ç§¯å¤§
- **è®­ç»ƒæ—¶é—´é•¿**ï¼šéœ€è¦è®­ç»ƒå¤§é‡å†³ç­–æ ‘
- **å¯è§£é‡Šæ€§å·®**ï¼šéš¾ä»¥ç†è§£æ•´ä¸ªæ£®æ—çš„å†³ç­–é€»è¾‘
- **å†…å­˜å ç”¨å¤§**ï¼šéœ€è¦å­˜å‚¨å¤šæ£µæ ‘
- **å¤–æ¨èƒ½åŠ›å¼±**ï¼šæ— æ³•é¢„æµ‹è¶…å‡ºè®­ç»ƒæ•°æ®èŒƒå›´çš„å€¼

---

## 9ï¸âƒ£ å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    class_weight='balanced',  # è‡ªåŠ¨è°ƒæ•´ç±»åˆ«æƒé‡
    random_state=42
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
```

### æ¡ˆä¾‹2ï¼šæˆ¿ä»·é¢„æµ‹

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

model = RandomForestRegressor(
    n_estimators=300,
    max_depth=20,
    min_samples_split=5,
    n_jobs=-1,
    random_state=42
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"RÂ² Score: {r2:.4f}")
```

---

## ğŸ”Ÿ ä¸å…¶ä»–ç®—æ³•å¯¹æ¯”

| ç®—æ³• | å‡†ç¡®ç‡ | è®­ç»ƒé€Ÿåº¦ | å¯è§£é‡Šæ€§ | æŠ—è¿‡æ‹Ÿåˆ | ç‰¹å¾é‡è¦æ€§ |
|------|--------|----------|----------|----------|------------|
| éšæœºæ£®æ— | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| å†³ç­–æ ‘ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­â­ |
| GBDT | â­â­â­â­â­ | â­â­ | â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| XGBoost | â­â­â­â­â­ | â­â­â­ | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |

---

## ğŸ¯ å…³é”®è¦ç‚¹

1. éšæœºæ£®æ—é€šè¿‡**é›†æˆå¤šæ£µå†³ç­–æ ‘**æé«˜é¢„æµ‹æ€§èƒ½
2. ç»“åˆ **Bagging**ï¼ˆBootstrap æŠ½æ ·ï¼‰å’Œ**éšæœºç‰¹å¾é€‰æ‹©**ä¸¤å¤§æŠ€æœ¯
3. æ˜¾è‘—é™ä½**è¿‡æ‹Ÿåˆé£é™©**ï¼Œæ¯”å•æ£µå†³ç­–æ ‘æ›´ç¨³å®š
4. èƒ½å¤Ÿè®¡ç®—**ç‰¹å¾é‡è¦æ€§**ï¼Œå¸®åŠ©ç‰¹å¾é€‰æ‹©å’Œè§£é‡Šæ¨¡å‹
5. **è¢‹å¤–è¯¯å·®**ï¼ˆOOB Errorï¼‰å¯ç”¨äºæ¨¡å‹è¯„ä¼°ï¼Œæ— éœ€é¢å¤–éªŒè¯é›†
6. å…³é”®è¶…å‚æ•°ï¼š**n_estimators**ï¼ˆæ ‘çš„æ•°é‡ï¼‰ã€**max_depth**ï¼ˆæœ€å¤§æ·±åº¦ï¼‰ã€**max_features**ï¼ˆç‰¹å¾æ•°é‡ï¼‰
7. é€‚ç”¨äº**åˆ†ç±»**å’Œ**å›å½’**ä»»åŠ¡ï¼Œåœ¨è®¸å¤šå®é™…é—®é¢˜ä¸­è¡¨ç°ä¼˜å¼‚

