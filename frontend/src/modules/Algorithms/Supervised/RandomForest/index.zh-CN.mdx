import { useState } from 'react';
import { useTranslation } from 'react-i18next';
import axios from 'axios';
import { Demo, ParameterSlider, Button, Chart, AlgorithmHeader, PlayIcon, ResetIcon } from '@common/modules/MDX/MDXComponents';
import { useLogger } from '@common/modules/Logger';
import { useExceptions, ExceptionType, ExceptionSeverity } from '@common/modules/Exceptions';

export const RandomForestDemo = () => {
  const { t } = useTranslation();
  const logger = useLogger();
  const { captureException } = useExceptions();
  const [nEstimators, setNEstimators] = useState(100);
  const [maxDepth, setMaxDepth] = useState(10);
  const [loading, setLoading] = useState(false);
  const [result, setResult] = useState(null);

  const runTraining = async () => {
    setLoading(true);
    logger.info('开始训练随机森林模型', { nEstimators, maxDepth });
    
    try {
      const response = await axios.post('/api/algorithms/supervised/random-forest/train', {
        n_estimators: nEstimators,
        max_depth: maxDepth,
      });
      setResult(response.data);
      logger.info('随机森林模型训练成功', { result: response.data });
    } catch (error) {
      logger.error('随机森林模型训练失败', { error: error.message, nEstimators, maxDepth });
      captureException(error, {
        type: ExceptionType.NETWORK,
        severity: ExceptionSeverity.HIGH,
        context: { 
          algorithm: 'random-forest',
          nEstimators,
          maxDepth,
          message: '训练失败，请检查后端服务是否正常运行'
        }
      });
    } finally {
      setLoading(false);
    }
  };

  const resetParameters = () => {
    setNEstimators(100);
    setMaxDepth(10);
    setResult(null);
  };

  return (
    <Demo title="⚡ 交互式实验">
      <div className="space-y-6">
        <div className="bg-gray-50 dark:bg-gray-800 p-4 rounded-lg">
          <h4 className="font-semibold mb-4 text-gray-900 dark:text-gray-100">📊 参数调整</h4>
          
          <ParameterSlider
            label="决策树数量 (N Estimators)"
            value={nEstimators}
            onChange={setNEstimators}
            min={10}
            max={500}
            step={10}
          />
          
          <ParameterSlider
            label="最大深度 (Max Depth)"
            value={maxDepth}
            onChange={setMaxDepth}
            min={1}
            max={30}
            step={1}
          />

          <div className="flex gap-3 mt-6">
            <Button onClick={runTraining} icon={PlayIcon} disabled={loading}>
              {loading ? '训练中...' : '运行训练'}
            </Button>
            <Button onClick={resetParameters} variant="secondary" icon={ResetIcon}>
              重置参数
            </Button>
          </div>
        </div>

        {result && (
          <div className="space-y-4">
            <div className="bg-green-50 dark:bg-green-900/20 p-4 rounded-lg">
              <h4 className="font-semibold mb-2 text-gray-900 dark:text-gray-100">🌲 训练结果</h4>
              <div className="grid grid-cols-2 gap-4 text-sm">
                <div>
                  <span className="text-gray-600 dark:text-gray-400">准确率 (Accuracy):</span>
                  <span className="ml-2 font-mono font-semibold">{result.accuracy?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">F1 Score:</span>
                  <span className="ml-2 font-mono font-semibold">{result.f1_score?.toFixed(4)}</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">训练时间:</span>
                  <span className="ml-2 font-mono font-semibold">{result.training_time?.toFixed(3)}s</span>
                </div>
                <div>
                  <span className="text-gray-600 dark:text-gray-400">树的数量:</span>
                  <span className="ml-2 font-mono font-semibold">{result.n_trees}</span>
                </div>
              </div>
            </div>

            {result.feature_importance && (
              <Chart
                data={result.feature_importance}
                layout={{
                  title: '特征重要性',
                  xaxis: { title: '重要性' },
                  yaxis: { title: '特征' },
                  height: 400,
                }}
              />
            )}
          </div>
        )}
      </div>
    </Demo>
  );
};

<AlgorithmHeader 
  title="随机森林 Random Forest"
  difficulty="intermediate"
  stars={3}
  time={30}
/>

## 1️⃣ 算法概览

**随机森林**（Random Forest）是一种基于**集成学习**（Ensemble Learning）的强大算法，通过构建多棵决策树并**投票表决**来提高预测性能。它结合了**Bagging**（Bootstrap Aggregating）和**随机特征选择**两大技术，显著降低了单棵决策树的过拟合风险。

### 核心思想

- 从训练集中**随机抽样**（有放回抽样，Bootstrap）构建多个子集
- 在每个子集上训练一棵决策树
- 每次分裂时只考虑**随机选择的特征子集**
- **多数投票**（分类）或**平均值**（回归）作为最终预测

### 应用场景

- 🏥 **疾病诊断**：综合多个特征进行疾病预测
- 💳 **信用评分**：评估贷款违约风险
- 📊 **股票预测**：预测股票价格走势
- 🎯 **推荐系统**：用户行为预测和内容推荐

---

## 2️⃣ 发展历史

- **1994年** - **Tin Kam Ho** 提出随机子空间方法（Random Subspace Method）
- **1996年** - **Leo Breiman** 提出 **Bagging** 算法（Bootstrap Aggregating）
- **2001年** - **Leo Breiman** 正式提出 **Random Forest** 算法，结合 Bagging 和随机特征选择
- **2003年** - Breiman 和 Cutler 完善随机森林理论，并推广应用
- **21世纪** - 随机森林成为 Kaggle 竞赛和工业界最受欢迎的算法之一

---

## 3️⃣ 数学原理

### Bagging（Bootstrap Aggregating）

从原始数据集 $D$ 中进行 $B$ 次**有放回随机抽样**，生成 $B$ 个子数据集 $D_1, D_2, ..., D_B$：

```math
D_b = \{(\mathbf{x}_i, y_i)\}, \quad i \sim Uniform(1, N)
```

在每个子集上训练一个基学习器（决策树）$h_b$。

### 随机特征选择

在每次节点分裂时，不考虑所有 $d$ 个特征，而是随机选择 $m$ 个特征（$m \ll d$）：

```math
m = \lfloor\sqrt{d}\rfloor \quad \text{(分类任务)}
```

```math
m = \lfloor d/3 \rfloor \quad \text{(回归任务)}
```

### 集成预测

**分类任务**：多数投票

```math
H(\mathbf{x}) = \arg\max_{y}\sum_{b=1}^{B}I(h_b(\mathbf{x}) = y)
```

**回归任务**：平均值

```math
H(\mathbf{x}) = \frac{1}{B}\sum_{b=1}^{B}h_b(\mathbf{x})
```

### 袋外误差（Out-of-Bag Error）

每个样本平均只出现在约 **63.2%** 的 Bootstrap 样本中（$1 - e^{-1} \approx 0.632$），剩余 **36.8%** 的样本可作为验证集：

```math
OOB\_Error = \frac{1}{N}\sum_{i=1}^{N}I(H_{OOB}^{(i)}(\mathbf{x}_i) \neq y_i)
```

---

## 4️⃣ 交互式实验 ⚡

调整随机森林的关键参数，观察对性能和训练时间的影响：

<RandomForestDemo />

---

## 5️⃣ 代码实现

### Python 实现（scikit-learn）

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import numpy as np
import pandas as pd

# 加载数据
from sklearn.datasets import load_iris
iris = load_iris()
X, y = iris.data, iris.target

# 划分数据集
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 创建随机森林分类器
model = RandomForestClassifier(
    n_estimators=100,        # 决策树数量
    max_depth=10,            # 最大深度
    min_samples_split=2,     # 分裂所需最小样本数
    min_samples_leaf=1,      # 叶节点最小样本数
    max_features='sqrt',     # 每次分裂考虑的特征数
    bootstrap=True,          # 是否使用 Bootstrap 抽样
    oob_score=True,          # 是否计算袋外误差
    n_jobs=-1,               # 并行计算
    random_state=42
)

# 训练模型
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)

# 评估
accuracy = accuracy_score(y_test, y_pred)
print(f"准确率: {accuracy:.4f}")
print(f"袋外误差: {1 - model.oob_score_:.4f}")

# 特征重要性
feature_importance = pd.DataFrame({
    'feature': iris.feature_names,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)
print("\n特征重要性:")
print(feature_importance)
```

### 随机森林回归

```python
from sklearn.ensemble import RandomForestRegressor

# 回归任务
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42
)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

---

## 6️⃣ 特征重要性分析

随机森林能够计算特征重要性，帮助理解哪些特征对预测最有影响：

### 基于不纯度减少（Gini Importance）

```math
Importance(X_j) = \frac{1}{B}\sum_{b=1}^{B}\sum_{t \in T_b}I(v_t = X_j) \cdot \Delta Gini_t
```

其中 $\Delta Gini_t$ 是节点 $t$ 分裂后基尼指数的减少量。

### Python 实现

```python
import matplotlib.pyplot as plt

# 获取特征重要性
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# 可视化
plt.figure(figsize=(10, 6))
plt.title("特征重要性")
plt.bar(range(X.shape[1]), importances[indices])
plt.xticks(range(X.shape[1]), [iris.feature_names[i] for i in indices], rotation=45)
plt.tight_layout()
plt.show()
```

### 基于排列重要性（Permutation Importance）

```python
from sklearn.inspection import permutation_importance

# 计算排列重要性
perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)

print("排列重要性:")
for i in perm_importance.importances_mean.argsort()[::-1]:
    print(f"{iris.feature_names[i]}: {perm_importance.importances_mean[i]:.4f} +/- {perm_importance.importances_std[i]:.4f}")
```

---

## 7️⃣ 超参数调优

### 网格搜索

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("最佳参数:", grid_search.best_params_)
print("最佳得分:", grid_search.best_score_)
```

### 随机搜索

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(50, 500),
    'max_depth': randint(5, 30),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', None]
}

random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_dist,
    n_iter=100,
    cv=5,
    random_state=42,
    n_jobs=-1
)

random_search.fit(X_train, y_train)
```

---

## 8️⃣ 优缺点分析

### ✅ 优点

- **高精度**：集成多个决策树，显著提升预测性能
- **抗过拟合**：通过随机性和集成降低方差
- **处理高维数据**：能有效处理大量特征
- **特征重要性**：自动评估特征对预测的贡献
- **鲁棒性强**：对缺失值和异常值不敏感
- **并行化**：树之间独立，可并行训练

### ❌ 缺点

- **模型复杂**：包含多棵树，模型体积大
- **训练时间长**：需要训练大量决策树
- **可解释性差**：难以理解整个森林的决策逻辑
- **内存占用大**：需要存储多棵树
- **外推能力弱**：无法预测超出训练数据范围的值

---

## 9️⃣ 实际应用案例

### 案例1：信用卡欺诈检测

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# 处理类别不平衡
model = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    class_weight='balanced',  # 自动调整类别权重
    random_state=42
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
```

### 案例2：房价预测

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

model = RandomForestRegressor(
    n_estimators=300,
    max_depth=20,
    min_samples_split=5,
    n_jobs=-1,
    random_state=42
)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"R² Score: {r2:.4f}")
```

---

## 🔟 与其他算法对比

| 算法 | 准确率 | 训练速度 | 可解释性 | 抗过拟合 | 特征重要性 |
|------|--------|----------|----------|----------|------------|
| 随机森林 | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 决策树 | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ |
| GBDT | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| XGBoost | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

---

## 🎯 关键要点

1. 随机森林通过**集成多棵决策树**提高预测性能
2. 结合 **Bagging**（Bootstrap 抽样）和**随机特征选择**两大技术
3. 显著降低**过拟合风险**，比单棵决策树更稳定
4. 能够计算**特征重要性**，帮助特征选择和解释模型
5. **袋外误差**（OOB Error）可用于模型评估，无需额外验证集
6. 关键超参数：**n_estimators**（树的数量）、**max_depth**（最大深度）、**max_features**（特征数量）
7. 适用于**分类**和**回归**任务，在许多实际问题中表现优异

